# EPT Identity Branch

This branch demonstrates several concepts which make use of our prior knowledge of the EPT paging mechanism in order to manipulate guest software. The pervious branch (EPT) demonstrated extremely granular identity mappings of 4KB pages within our EPT. This branch, as the name suggests, arranges an EPT mapping of all of the physical memory on the guest target machine; primarily by utilizing 2MB large page mappings (PDEs). Helper routines have been added to obtain memory types for these large page PDEs via MTRRs, selectively split PDEs, and perform TLB shootdowns. Additionally, to demonstrate control over guest software using our system identity mappings, our VMM now demonstrates a technique known as EPT *splitting*.

EPT splitting, as seen in the screenshot below, involves changing the mapping of a PTE depending on the type of access that is requested by guest software. For instance, if a default PTE has the read, write, and execution bits set, one may end up removing the execution bit on a page of code. This would cause the hypervisor to break into your predefined the exit handler upon execution reaching your target page. At this point, a different page of code may be mapped in place of the original code, this time with only the execute permission bit set. This process would then be repeated upon read/write accesses to the newly-placed execute-only PTE. The result of these operations is a page of hidden code that is not detectable by the guest software; as all read operations on this code region would access the original, now exclusively read/write mapping as opposed to the modified executable mapping (and vice versa).

![EPT Splitting Demo](./media/demo.png)

[Compare this branch to it's base \(EPT\)](https://github.com/calware/HV-Playground/compare/EPT...EPTIdentity)

# Identity Mapping and Large Pages

Let's talk for a moment about identity mapping. In the previous writeup (seen in the EPT branch), we showed an example of how one would go about translating a virtual address to a physical address manually with WinDBG. To do the same thing with large page allocations (which you will need to be familiar with in order to test your mappings), you would simply stop at either a PDPTE, or a PDE, and examine the page size (PS) bit (7). For a PDPTE with the PS bit set, instead of map a PDT, the PDPTE will map a 1GB allocation (holding actual data), which is indexed at 1-byte granularity. In order to index a 1GB allocation with 1-byte granularity, 30-bits are required. These 30-bits would be obtained via the last 30-bits of the target physical address. This process is the same with 2MB large page PDEs. These large page PDEs, instead of mapping PTs, will map 2MB pages (holding actual data), which use the last 21-bits of the target physical address to index at 1-byte granularity.

So at this point you may be wondering: why would we want to use large page mappings instead of just mapping down to 4KB PTEs. Mappings which translate down to 4KB PTEs would allow for more granular modifications of the paging tables, and it's something we're already familiar with. The reason for using these large page mappings stems from the fact that mapping all of the available system memory down to respective 4KB PTEs will use a considerable amount of space, and take a lot of time. That sort of granular control over 4KB PTEs is still something that is very much sought-after though, and this brings us to an idea that will be expanded on further in the below text. We will have to selectively split these large pages into PTs holding PTEs which map 4KB pages.

Additionally, one may wonder why it is we use 2MB PDEs instead of 1GB PDPTEs. This is simply because virtualization software—in my case VMware—doesn't support 1GB large pages.

# EPT Identity Mapping: Methodology

There are a couple different ways you can create a 1:1 mapping for an underlying operating system using EPT. Typically, what you'll see what is the case in Alex Ionescu's SimpleVisor, or Sina Karvandi's Hypervisor From Scratch, which is the mapping of just one *full* PDPT. As you know from the pervious branch, each 8-byte PDPTE can map 1GB of memory (be that mapping a large page entry, or another series of page tables), and since each paging table is 4KB in size, you end up with a PDPT capable of mapping 512GB of memory. Both of the previously mentioned projects, among many others, choose to fill an entire PDPT to map 512GB of memory, assuming that the actual host memory will never exceed this amount. This method could of course be improved upon by attempting to map a range of physical memory that is more reflective of the host's actual limitations. An example of an identity mapping routine which doesn't rely on filling a whole 512GB PDPT is DarthTon's HyperBone. Instead of mapping 512 PDPTEs, HyperBone chose to base their identity map on the Windows physical memory map. This comes with it's own challenges, however, as the routine used to obtain data on the operating system's memory map (`MmGetPhysicalMemoryRanges`) is undocumented; and therefore subject to change. In addition to the uncertainties posed by utilizing these undocumented API functions, several regions of memory must be included in your identity mappings which aren't present the mappings returned by `MmGetPhysicalMemoryRanges`. One of these ranges is for the local APIC, and another is something I haven't yet been able to define.

There is also the problem of how one goes about discerning the memory type of a region of physical memory. For some regions of memory, the type will have to indicate uncacheable (UC) memory, versus something like write-back (WB) memory; there's usually a reason for the system to be using one over the other, so it's important to get this right. The WB and UC memory types are also only two of the possible five memory types. Projects like SimpleVisor overcome this issue by referring to the processor's MTRRs; which can allow one to reliably determine the memory types for physical ranges. In the case of HyperBone however, there is an assumption made about memory ranges reported by `MmGetPhysicalMemoryRanges`. Chiefly, the code assumes that all of these ranges will fall in write-back memory.

For my identity mapping routines, I choose to use the same technique as SimpleVisor, but with a couple changes. Instead of mapping 512GB of memory, I attempt to calculate a more appropriate and typical system-memory size in the `GetOptimalPhysMemMapSize` function (seen in the `Memory` source files). The max physical memory mapping size is 64GB, and compatible memory types are deduced by utilizing MTRRs. My code also performs checks that I believe better equip the EPT mappings to handle memory types that are between WB and UC.

# Splitting The Large Pages, and TLB shootdowns

At some point, as we mentioned above, you may wish to modify the page mappings within your EPT in a granular fashion. A good example of this would be to modify the permission bits on a target page to achieve the EPT splitting technique we discussed in the opening of this document. Given our array of large page mappings which make up our complete identity map, the most we can reduce a guest-physical address down to is a one 2MB PDE (assuming the allocation doesn't gap a boundary). The next step is to transform this 2MB large page PDE into a PDE mapping a PT which holds 512 PTEs mapping 4KB regions. This sounds a lot more complicated than it really is. Usually this just entails a simple `for` loop, which runs through a PT and sets PTE base addresses to the initial base address of the large page, and then 4KB past whatever the PTE's last base address was. Finally, in order to obtain the target PTE in your EPT, you just translate the target guest-physical address as you would normally. There may be some challenges in allocating PTs to point large-page PDEs to, as the routines used to allocate memory are not supported at an IRQL greater than HIGH_LEVEL. In cases where you can't defer allocations of PTs to point large-page PDEs to, a cache is usually used. You can view all of these capabilities with additional notes in my `EPT` source files.

It's important to understand: paging tables and translations derived therefrom usually result in entries within the logical processor's TLB. In changing these paging structures, the responsibility falls on you to invalidate these entries in the TLB via the `INVEPT` instruction. This can be a tricky venture though, as some systems don't support what's called an *all-context* invalidation; which would do exactly what it is you would want to accomplish. Instead, some systems—and I'm uncertain how common this is—support only *single-context* invalidations; which are only effective on the logical processor who executed the instruction. This poses even more of a challenge when you consider that you can't easily invalidate the TLB mappings for other logical processors via generating inter-processor interrupts; as all interrupts are disabled, and your IRQL is effectively greater than that of HIGH_LEVEL. Thankfully, given the structure of our project, only one thread runs the guest software, so this is a non-issue.
